{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model\n",
    "\n",
    "To train a model you should specify the arguments, for example:\n",
    "\n",
    "```bash\n",
    "python run_metonymy_resolution.py \\\n",
    "--data_dir ../data \\\n",
    "--train_file conll_train.json \\\n",
    "--predict_file conll_test.json \\\n",
    "--output_dir ../output \\\n",
    "--do_train  \\\n",
    "--do_eval \\\n",
    "--do_mask\n",
    "```\n",
    "\n",
    "# You can also use our pretrained model\n",
    "\n",
    "Download from, https://drive.google.com/file/d/1PCXkEFyK5OALQbF_64J6jSGO0IUjbYuf/view?usp=sharing\n",
    "\n",
    "Unzip and put it to ``model_folder`` (you local path).\n",
    "\n",
    "This is a pretrained bert-base-uncased model, use ./data/conll_train.json."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pretrained model from ``model_folder``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"./src\")\n",
    "from utils_metonymy import *\n",
    "\n",
    "model_class = BertForWordClassification\n",
    "tokenizer_class = BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = './output' # specify which dir the model been saved.\n",
    "\n",
    "model = model_class.from_pretrained(model_folder)\n",
    "tokenizer = tokenizer_class.from_pretrained(model_folder, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def single_detect(example):\n",
    "    tgt_word = \" \".join(example[\"sentence\"][example[\"pos\"][0]:example[\"pos\"][1]])\n",
    "    sent = \" \".join(example[\"sentence\"])\n",
    "    tmp = copy.deepcopy(example)\n",
    "    inputs = convert_single_example_to_input(tmp, tokenizer)\n",
    "    \n",
    "    model.zero_grad()\n",
    "    model.eval()\n",
    "\n",
    "    logits = model(**inputs)[0]\n",
    "\n",
    "    preds = logits.detach().cpu().numpy()\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "\n",
    "    label_map = {0:'literal',1:'metonymy'}\n",
    "    \n",
    "    print(f'------------------------------')\n",
    "    print(f'Target word:   {tgt_word}')\n",
    "    print(f'Sentence:      {sent}')\n",
    "    print(f'Prediction:    {label_map[preds[0]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input example & Run the model on the input example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Target word:   ROMANIA\n",
      "Sentence:      SOCCER - ROMANIA BEAT LITHUANIA IN UNDER-21 MATCH.\n",
      "Prediction:    metonymy\n"
     ]
    }
   ],
   "source": [
    "example = {'sentence': ['SOCCER', '-', 'ROMANIA', 'BEAT', 'LITHUANIA', 'IN', 'UNDER-21', 'MATCH.'],\n",
    "           'pos': [2, 3]}\n",
    "single_detect(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Metonymy Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def end2end_detect(sentence):\n",
    "    tokens = nlp(sentence)\n",
    "    pos = [-1,-1]\n",
    "    examples = []\n",
    "    token_text = list(map(lambda x: x.text, tokens))\n",
    "    for i,token in enumerate(tokens):\n",
    "        if token.ent_iob_ == 'O':\n",
    "            if pos == [-1, -1]:\n",
    "                continue\n",
    "            else:\n",
    "                pos[1] = i\n",
    "                examples.append({'sentence': token_text, 'pos': pos})\n",
    "                pos = [-1,-1]\n",
    "        elif token.ent_iob_ == 'B' and token.ent_type_ == 'GPE':\n",
    "            pos = [i,i]\n",
    "        elif token.ent_iob_ == 'I' and token.ent_type_ == 'GPE':\n",
    "            pos[1] = i\n",
    "            \n",
    "    for example in examples:\n",
    "        single_detect(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Target word:   Los Angeles\n",
      "Sentence:      Los Angeles lost in the semi - final .\n",
      "Prediction:    metonymy\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Los Angeles lost in the semi-final.\"\n",
    "end2end_detect(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Target word:   Moscow\n",
      "Sentence:      Moscow talks to Beijing .\n",
      "Prediction:    metonymy\n",
      "------------------------------\n",
      "Target word:   Beijing\n",
      "Sentence:      Moscow talks to Beijing .\n",
      "Prediction:    metonymy\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Moscow talks to Beijing.\"\n",
    "end2end_detect(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Target word:   Germany\n",
      "Sentence:      I used to live in Germany .\n",
      "Prediction:    literal\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I used to live in Germany.\"\n",
    "end2end_detect(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
